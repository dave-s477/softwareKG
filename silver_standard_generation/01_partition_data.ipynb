{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition Data\n",
    "\n",
    "We handle a large amount of new annotation data in Snorkel.\n",
    "The problem is that Snorkel extracts a huge amount of structured information about all sentences and ngrams it calculates. \n",
    "Therefore, working on all data at once would exhaust the available memory quite quickly.\n",
    "To work around this we split the new data into smaller chuncks and process each set individually.\n",
    "But to initialize the model in each database we also need to copy the base data to each new data chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, makedirs\n",
    "from os.path import exists\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a list of the files we actually need to train and test Snorkel and will only copy the needed annotated files.\n",
    "Now we just load all lists of files we are going to partition in the next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = listdir('data/new_data_and_sosci/')\n",
    "new_files = [x for x in file_list if not x.startswith('sent')]\n",
    "random.seed(42)\n",
    "random.shuffle(new_files)\n",
    "train_files = [x for x in file_list if x.startswith('sent')]\n",
    "train_annotation = [x for x in listdir('data/sosci') if x.endswith('.ann')]\n",
    "with open('sosci_train_dev_test_split.json', 'r') as sosci_data_json:\n",
    "    train_dev_test_split = json.load(sosci_data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will perform the actual split of the silver standard into a number of fixed buckets and directly copy the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'sosci_ssc_' \n",
    "BUCKETS_TO_BUILD = 64\n",
    "BUCKETS_TO_PROCESS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_sample = math.ceil(len(new_files)/BUCKETS_TO_BUILD)\n",
    "for x in range(BUCKETS_TO_PROCESS):\n",
    "    makedirs('data/{}{}'.format(output_name, x))\n",
    "    files_in_sample = new_files[x*num_per_sample:(x+1)*num_per_sample]\n",
    "    for f in files_in_sample:\n",
    "        copyfile('data/new_data_and_sosci/{}'.format(f), 'data/{}{}/{}'.format(output_name,x,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also need to copy the annotated files into all created buckets.\n",
    "\n",
    "Here we use a certain to distinguish between training data and testing data. \n",
    "If we just want to train the model we do not actually need to import the testing set, because this will only cost us memory space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test_set = 'test'\n",
    "\n",
    "#makedirs('data/{}{}'.format(output_name, '0'))\n",
    "for x in range(BUCKETS_TO_PROCESS):\n",
    "    for f in train_files:\n",
    "        file_name = f.split('.txt')[0]\n",
    "        if file_name in train_dev_test_split['train']:\n",
    "            copyfile('data/sosci/{}'.format(f), 'data/{}{}/{}'.format(output_name,x,f))\n",
    "        elif file_name in train_dev_test_split['devel']:\n",
    "            if train_or_test_set == 'test':\n",
    "                copyfile('data/sosci/{}'.format(f), 'data/{}{}/{}'.format(output_name,x,f))\n",
    "        elif file_name in train_dev_test_split['test']:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Error: File {} was not in File split. This should not happen.\".format(file_name))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly import our annotation in Snorkel from BRAT format. \n",
    "But for that we need to pass the annotation along with the base files.\n",
    "We create this data structure in a separate folder, because otherwise it will lead to trouble when importing the plain text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('data/{}annotation'.format(output_name)): \n",
    "    makedirs('data/{}annotation'.format(output_name))\n",
    "for f in train_files:\n",
    "    file_name = f.split('.txt')[0]\n",
    "    if file_name in train_dev_test_split['train']:\n",
    "        copyfile('data/sosci/{}'.format(f), 'data/{}annotation/{}'.format(output_name, f))\n",
    "        copyfile('data/sosci/{}'.format(f.split('.txt')[0]+'.ann'), 'data/{}annotation/{}'.format(output_name, file_name+'.ann'))\n",
    "    elif file_name in train_dev_test_split['devel']:\n",
    "        if train_or_test_set == 'test':\n",
    "            copyfile('data/sosci/{}'.format(f), 'data/{}annotation/{}'.format(output_name, f))\n",
    "            copyfile('data/sosci/{}'.format(f.split('.txt')[0]+'.ann'), 'data/{}annotation/{}'.format(output_name, file_name+'.ann'))\n",
    "    elif file_name in train_dev_test_split['test']:\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Error: File {} was not in File split. This should not happen.\".format(file_name))\n",
    "copyfile('data/sosci/annotation.conf', 'data/{}annotation/annotation.conf'.format(output_name))      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
