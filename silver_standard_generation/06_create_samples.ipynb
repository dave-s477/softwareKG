{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Samples for Silver Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_standard_location = '../data/generated_silver_data/'\n",
    "silver_standard_files = [x for x in listdir(silver_standard_location) if not \"cleaned\" in x and x.startswith('silver_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_file(file_name):\n",
    "    with open(file_name, 'r') as in_file, open(file_name.split(\".csv\")[0]+'_cleaned.csv', 'w') as out_file:\n",
    "        text = in_file.read()\n",
    "        out_text = re.sub(r'\\n\\[', ' [', text)\n",
    "        out_text = re.sub(r'\\n\\(', ' (', out_text)\n",
    "        out_text = re.sub(r'\\n\\)', ' )', out_text)\n",
    "        out_text = re.sub(r'\\n\\t', ' ', out_text)\n",
    "        out_text = re.sub(r'\\n\\*', ' *', out_text)\n",
    "        out_text = re.sub(r'\\n;', ' ;', out_text)\n",
    "        out_text = re.sub(r'\\n\"', ' \"', out_text)\n",
    "        out_text = re.sub(r'\\n%', ' %', out_text)\n",
    "        out_text = re.sub(r'\\n[^\\d]', ' ', out_text)\n",
    "        out_file.write(out_text)\n",
    "for f in silver_standard_files:\n",
    "    cleanup_file(join(silver_standard_location, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(file_list, silver_standard_location):\n",
    "    df = pd.DataFrame(columns=['span_id','span','beg_off','end_off','sent_id','sent','doc_id','doc','marg'])\n",
    "    for f in file_list:\n",
    "        print(\"Working on file {}\".format(f))\n",
    "        with open(join(silver_standard_location, f.split('.csv')[0]+\"_cleaned.csv\"), 'r') as csv:\n",
    "            lines = csv.readlines()\n",
    "            for idx, line in enumerate(lines[1:]):\n",
    "                span_id, line = re.split(r',', line, maxsplit=1)\n",
    "                span, line = re.split(r',', line, maxsplit=1)\n",
    "                span = span.strip('\"')\n",
    "                beg_off, line = re.split(r',', line, maxsplit=1)\n",
    "                end_off, line = re.split(r',', line, maxsplit=1)\n",
    "                sent_id, line = re.split(r',', line, maxsplit=1)\n",
    "                sent, line = re.split(r'\",', line, maxsplit=1)\n",
    "                sent = sent.strip('\"')\n",
    "                doc_id, line = re.split(r',', line, maxsplit=1)\n",
    "                doc, line = re.split(r'\",', line, maxsplit=1)\n",
    "                doc = doc.strip('\"')\n",
    "                marg = re.split(r',', line, maxsplit=1)[0].rstrip('\\n')\n",
    "                entry = {\n",
    "                    'span_id': span_id,\n",
    "                    'span': span,\n",
    "                    'beg_off': beg_off,\n",
    "                    'end_off': end_off,\n",
    "                    'sent_id': sent_id,\n",
    "                    'sent': sent,\n",
    "                    'doc_id': doc_id,\n",
    "                    'doc': doc,\n",
    "                    'marg': marg\n",
    "                }\n",
    "                df = df.append(entry, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe(silver_standard_files, silver_standard_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still the possibility that overlapping samples have been extracted. If that is the case we systematically remove them and keep the longest samples that were extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_overlapping(df):\n",
    "    df['overlapped'] = False\n",
    "    doc_names = set(df['doc'])\n",
    "    for doc_id in doc_names:\n",
    "        doc_sentences = df.loc[df.doc == doc_id]\n",
    "        for sent_id in set(doc_sentences['sent_id']):\n",
    "            multi_sentences = doc_sentences.loc[doc_sentences.sent_id == sent_id]\n",
    "            if len(multi_sentences) < 2:\n",
    "                continue\n",
    "            for index_test, row_test in multi_sentences.iterrows():\n",
    "                for index_other, row_other in multi_sentences.iterrows():\n",
    "                    if index_test == index_other:\n",
    "                        continue\n",
    "                    elif row_other['beg_off'] <= row_test['beg_off'] and row_other['end_off'] >= row_test['end_off']:\n",
    "                        df.loc[index_test, 'overlapped'] = True\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mark_overlapping(df)\n",
    "df = df.loc[df.overlapped == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we need to remove the set of files used in the Snorkel training from the silver standard so we do not have a bias when we evaluate later on. Because the sentences are randomly split, we remove all files from the snorkel train and devel set as well as from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_training_files = [re.sub(r'/', '_', x.split('.xml')[0]) for x in listdir('../data/XML')]\n",
    "for original_file in original_training_files:\n",
    "    df = df.loc[df.doc != original_file]\n",
    "df.to_csv(\"silver_standard.csv\")\n",
    "# To load: df = pd.read_csv(\"pandas_silver_standard_production.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query negative samples\n",
    "\n",
    "At this point it is easiest to perform the operation in the database because we need additional information.\n",
    "Basically we need to:\n",
    "1. travers each database in the silver data split\n",
    "2. travers each document in the database\n",
    "3. travers each sentence in the document \n",
    "4. write the sentence to the negative samples if the doc-sentence combination is not yet in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pandas_silver_standard.csv', low_memory=False)\n",
    "doc_names = set(df['doc'])\n",
    "doc_fails = [x for x in doc_names if not x.startswith('10.1371')]\n",
    "for fail in doc_fails:\n",
    "    df = df.drop(df[df.doc == fail].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_standard_files = [re.sub(r'data', 'standard', x.split('_training')[0]) for x in listdir('generated_silver_data/') if x.startswith('silver')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(database_name):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user=\"ds626\",\n",
    "                                      password=\"snorkel\",\n",
    "                                      host=\"127.0.0.1\",\n",
    "                                      port=\"5432\",\n",
    "                                      database=database_name)\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        query = \"select * from document\"\n",
    "\n",
    "        cursor.execute(query)\n",
    "        documents = cursor.fetchall()\n",
    "        new_docs = []\n",
    "        for row in documents:\n",
    "            if not row[1].startswith('sent'):\n",
    "                new_docs.append([row[0], row[1]])\n",
    "        return new_docs\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error fetching data from PostgreSQL table\", error)\n",
    "\n",
    "    finally:\n",
    "        if (connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            \n",
    "def get_sentences(database_name, document):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user=\"ds626\",\n",
    "                                      password=\"snorkel\",\n",
    "                                      host=\"127.0.0.1\",\n",
    "                                      port=\"5432\",\n",
    "                                      database=database_name)\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        query = \"select id,document_id,words,text from sentence where document_id = %s\"\n",
    "\n",
    "        cursor.execute(query, (document,))\n",
    "        sents = cursor.fetchall()\n",
    "        new_sents = []\n",
    "        for row in sents:\n",
    "            new_sents.append([row[0], row[1], row[2], row[3]])\n",
    "        return new_sents\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error fetching data from PostgreSQL table\", error)\n",
    "\n",
    "    finally:\n",
    "        if (connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            \n",
    "def generate_negative_samples(silver_standard_files, output_name):\n",
    "    with open(join('data', '{}_data.txt'.format(output_name)), \"w\") as out_data, open(join('data', '{}_labels.txt'.format(output_name)), \"w\") as out_labels:\n",
    "        for silver_data_split in silver_standard_files:\n",
    "            silver_docs = get_docs(silver_data_split)\n",
    "            for document in silver_docs:\n",
    "                doc_id = document[1]\n",
    "                sents = get_sentences(silver_data_split, document[0])\n",
    "                for sent in sents:\n",
    "                    sent_id = sent[0]\n",
    "                    matching_df_sents = df.loc[(df.doc == str(doc_id)) & (df.sent_id == sent_id)]\n",
    "                    if len(matching_df_sents) < 1 and not sent[3].startswith('Figure data removed from full text') and not sent[3].startswith('Figure identifier and caption:')  and not sent[3].startswith('Table data removed from full text') and not sent[3].startswith('Table identifier and caption:'):\n",
    "                        # found an unknown sample\n",
    "                        word_string = ''\n",
    "                        label_string = ''\n",
    "                        for word in sent[2]:\n",
    "                            if word == '\\n':\n",
    "                                word_string += '\\n'\n",
    "                                label_string += '\\n'\n",
    "                                out_data.write(word_string)\n",
    "                                out_labels.write(label_string)\n",
    "                                word_string = ''\n",
    "                                label_string = ''\n",
    "                            else:\n",
    "                                word_string += ' ' + word\n",
    "                                label_string += ' O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_negative_samples(silver_standard_files, 'neg_silver_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform positive samples to BIO \n",
    "\n",
    "Here we have to work sentence-wise again. So we iterate all documents, all sentences and than transform each sentence into BIO including all spans that need to be annotated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_database_mapping():\n",
    "    mapping = {}\n",
    "    silver_sets = [x for x in listdir('data/') if x.startswith('silver_data')]\n",
    "    for silver_set in silver_sets:\n",
    "        new_files = [x for x in listdir(join('data', silver_set)) if not x.startswith('sent')]\n",
    "        for f in new_files:\n",
    "            mapping[f.split('.txt')[0]] = re.sub('data', 'standard', silver_set)\n",
    "    return mapping\n",
    "    \n",
    "def get_words(database_name, document, sentence):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user=\"ds626\",\n",
    "                                      password=\"snorkel\",\n",
    "                                      host=\"127.0.0.1\",\n",
    "                                      port=\"5432\",\n",
    "                                      database=database_name)\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        query = \"SELECT text, words FROM sentence, document WHERE sentence.document_id=document.id AND document.name=%s AND sentence.id=%s;\"\n",
    "\n",
    "        cursor.execute(query, (document,sentence))\n",
    "        sents = cursor.fetchall()\n",
    "        text, words = sents[0]\n",
    "        return text, words\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error fetching data from PostgreSQL table\", error)\n",
    "\n",
    "    finally:\n",
    "        if (connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "def create_pos_bio(df, name, document_database_mapping):\n",
    "    with open(join('data', '{}_data.txt'.format(name)), \"w\") as out_data, open(join('data', '{}_labels.txt'.format(name)), \"w\") as out_labels:\n",
    "        doc_names = set(df['doc'])\n",
    "        for idx, doc_id in enumerate(doc_names):\n",
    "            if idx % 500 == 0:\n",
    "                print(\"Working on doc {}: {}\".format(idx, doc_id))\n",
    "            doc_sentences = df.loc[df.doc == doc_id]\n",
    "            for sent_id in set(doc_sentences['sent_id']):\n",
    "                words_string = ''\n",
    "                labels_string = ''\n",
    "                text, words = get_words(document_database_mapping[doc_id], doc_id, sent_id)\n",
    "                multi_sentences = doc_sentences.loc[doc_sentences.sent_id == sent_id]\n",
    "                spans = []\n",
    "                for index_test, row_test in multi_sentences.iterrows():\n",
    "                    beg_off = int(row_test['beg_off'])\n",
    "                    end_off = int(row_test['end_off'])\n",
    "                    span = row_test['span']\n",
    "                    text_span = text[beg_off:end_off+1]\n",
    "                    #if span != text_span:\n",
    "                    #    print(\"Error: text spans do not match: {} and {}. In {} sentence {}\".format(span, text_span, doc_id, sent_id))\n",
    "                    spans.append([span, beg_off, end_off])\n",
    "                previous_off = 0\n",
    "                current_off = 0\n",
    "                remaining_text = text\n",
    "                for word in words:\n",
    "                    if word == ' ':\n",
    "                        pass\n",
    "                    elif word == '\\n':\n",
    "                        words_string += '\\n'\n",
    "                        labels_string += '\\n'\n",
    "                        out_data.write(words_string)\n",
    "                        out_labels.write(labels_string)\n",
    "                    else:\n",
    "                        previous_off = current_off\n",
    "                        words_string += ' ' + word\n",
    "                        current_off += len(word) \n",
    "                        span_matched = False\n",
    "                        for s in spans:\n",
    "                            if previous_off == s[1]:\n",
    "                                # begin\n",
    "                                labels_string += ' B-software'\n",
    "                                span_matched = True\n",
    "                            elif previous_off >= s[1] and previous_off <= s[2]:\n",
    "                                # inside\n",
    "                                labels_string += ' I-software'\n",
    "                                span_matched = True\n",
    "                        if not span_matched:\n",
    "                            labels_string += ' O'\n",
    "                        #print(word)\n",
    "                        #print(len(word))\n",
    "                        #print(text)\n",
    "                        #print(words_string)\n",
    "                        #print(remaining_text)\n",
    "                        #print(text)\n",
    "                        remaining_text = remaining_text.split(word, maxsplit=1)[1]\n",
    "                        current_off += len(remaining_text) - len(remaining_text.lstrip(' '))\n",
    "                        remaining_text = remaining_text.lstrip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_database_mapping = create_document_database_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pos_bio(df, 'pos_silver_samples', document_database_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
