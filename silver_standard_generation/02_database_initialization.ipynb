{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Database Initialization\n",
    "\n",
    "Snorkel handles all data in a database. In this file we will initialize the database we are working on. \n",
    "We import two annotated datasets\n",
    "- **Train set** used to create the rule set for the generative model\n",
    "- **Test set** used to evaluate the quality of the rules\n",
    "\n",
    "And we also import the new data that is to be tagged by Snorkel next. \n",
    "\n",
    "First we initialize Snorkel with a Session from which we access the database and the candidate scheme (software) we want to extract. \n",
    "\n",
    "We are using Postgres as a database managment system. So we need to make sure the database is initilized in Postgres so Snorkel can find it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "\n",
    "from glob import glob\n",
    "from shutil import copy\n",
    "\n",
    "BASE_NAME = 'sosci_ssc_0' \n",
    "DATABASE_NAME = 'sosci_ssc_0' \n",
    "LABELS_NAME = 'sosci_ssc_annotation' \n",
    "os.environ['SNORKELDB'] = 'postgres://snorkel:snorkel@localhost/' + DATABASE_NAME\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "session = SnorkelSession()\n",
    "software = candidate_subclass('software', ['software'])\n",
    "\n",
    "set_mapping = {\n",
    "    'train': 0, \n",
    "    'test': 1,\n",
    "    'new': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we select all documents we want to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from snorkel.parser import TextDocPreprocessor, CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.models import Document, Sentence\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import RegexMatchEach, RegexMatchSpan\n",
    "\n",
    "ngrams_one = Ngrams(n_max=6)\n",
    "software_matcher = RegexMatchEach(rgx=r'.*', longest_match_only=False)\n",
    "cand_extractor = CandidateExtractor(software, [ngrams_one], [software_matcher])\n",
    "\n",
    "doc_preprocessor = TextDocPreprocessor('../data/{}/'.format(BASE_NAME))  \n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "corpus_parser.apply(doc_preprocessor, parallelism=60)\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually build all possible ngrams on the data up to a max ngram count of 6 and already calculate all Spacy features which are built in for Snorkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('../data/sosci_train_dev_test_split.json', 'r') as sosci_data_json:\n",
    "    train_dev_test_split = json.load(sosci_data_json)\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents = set()\n",
    "new_sents = set()\n",
    "\n",
    "for _, doc in enumerate(docs):\n",
    "    if doc.name.startswith('sent'):\n",
    "        if doc.name in train_dev_test_split['train']:\n",
    "            for s in doc.sentences:\n",
    "                train_sents.add(s)\n",
    "        elif doc.name in train_dev_test_split['devel']:\n",
    "            for s in doc.sentences:\n",
    "                dev_sents.add(s)\n",
    "    else: \n",
    "        for s in doc.sentences:\n",
    "            new_sents.add(s)\n",
    "\n",
    "print(\"Working on \" + str(len(train_sents)) + \" training samples.\")  \n",
    "print(\"and on \" + str(len(dev_sents)) + \" testing samples.\")\n",
    "print(\"The set of new sentences contain {} sentences.\".format(len(new_sents)))\n",
    "\n",
    "for i, sents in enumerate([train_sents, dev_sents, new_sents]):\n",
    "    cand_extractor.apply(sents, split=i, parallelism=60)\n",
    "    print(\"Number of candidates:\", session.query(software).filter(software.split == i).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to also import the annotate labels which we do with an adjusted version of Snorkels BRAT importer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from util.brat_import import BratAnnotator\n",
    "\n",
    "brat = BratAnnotator(session, software, encoding='utf-8') \n",
    "train_cands = session.query(software).filter(software.split!=set_mapping['new']).all()\n",
    "brat.import_gold_labels(session, \"../data/{}/\".format(LABELS_NAME), train_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we split up the files we need to let this process run on each individual fraction of the data. We therefore also wrote a script that performs the same operations on a variable input to automate this process.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
