{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Supervision\n",
    "\n",
    "Create labeling functions based on distant supervision. For this purpose we will build a knowledge base containing known software names, which is used as a basis for extracting software mentions (hence the distant supervising). \n",
    "We work with a software dictionary (positive hints) queried from wikidata (and libraries.io). Additionally we use a dictionary of known english words as negative hints.\n",
    "\n",
    "## Base setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from shutil import copy\n",
    "from difflib import SequenceMatcher\n",
    "from functools import partial, update_wrapper \n",
    "\n",
    "BASE_NAME = 'sosci_ssc_0' \n",
    "DATABASE_NAME = 'sosci_ssc_0' \n",
    "LABELS_NAME = 'sosci_ssc_annotation' \n",
    "os.environ['SNORKELDB'] = 'postgres://snorkel:snorkel@localhost/' + DATABASE_NAME\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from itertools import product\n",
    "\n",
    "set_mapping = {\n",
    "    'train': 0, \n",
    "    'test': 1,\n",
    "    'new': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SnorkelSession()\n",
    "software = candidate_subclass('software', ['software'])\n",
    "devel_gold_labels = load_gold_labels(session, annotator_name='gold', split=set_mapping['train'])\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    contains_token, get_between_tokens, get_doc_candidate_spans,\n",
    "    get_left_tokens, get_matches, get_right_tokens, \n",
    "    get_sent_candidate_spans, get_tagged_text, get_text_between, \n",
    "    get_text_splits, is_inverted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cands = session.query(software).filter(software.split==set_mapping['train']).all()\n",
    "test_labels = load_gold_labels(session, annotator_name=\"gold\", split=set_mapping['train'])\n",
    "scorer = MentionScorer(test_cands, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "**Important**: the `test_LF` function is not imported, because it has hard coded queries and does not evaluate the results in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "\n",
    "The details of the used WikiData query can be found at `./distant_supervision/wiki_data.md`.\n",
    "\n",
    "We use the standard english dictionary Duck et al. used and the acronym dictionary to possibly exclude false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_data_software_list_01_09 = pd.read_csv(\"distant_supervision/wikidata_query_result.csv\")\n",
    "known_software_01_09 = set(wiki_data_software_list_01_09['itemLabel'].tolist())\n",
    "known_software_altNames_01_09 = set(wiki_data_software_list_01_09.dropna(subset=['abbreviation'])['abbreviation'].tolist())\n",
    "known_software_01_09 = known_software_01_09.union(known_software_altNames_01_09)\n",
    "to_remove_01_09 = []\n",
    "known_software_lower_01_09 = set()\n",
    "for soft in known_software_01_09:\n",
    "    if re.match(r'Q\\d[1,10]', soft):\n",
    "        to_remove_01_09.append(soft)\n",
    "    elif len(soft) > 2:\n",
    "        known_software_lower_01_09.add(soft.lower())\n",
    "for wrong_software_name in to_remove_01_09:\n",
    "    known_software_01_09.remove(wrong_software_name)\n",
    "print(\"Loaded a list of \" + str(len(known_software_01_09)) + \" unique software names from Wikidata.\")\n",
    "\n",
    "duck_dict = set()\n",
    "duck_dict_first_char_upper = set()\n",
    "duck_dict_lower = set()\n",
    "with open('distant_supervision/english.dic', encoding='iso8859-1') as eng_dic:\n",
    "    for line in eng_dic:\n",
    "        duck_dict.add(line.split('\\n')[0])\n",
    "        duck_dict_lower.add(line.split('\\n')[0].lower())\n",
    "        duck_dict_first_char_upper.add(line.split('\\n')[0])\n",
    "        duck_dict_first_char_upper.add(line.split('\\n')[0].capitalize())\n",
    "print(\"Duck dict contained \" + str(len(duck_dict)) + \" English words.\")\n",
    "print(\"and in Duck capitalized dict are \" + str(len(duck_dict_first_char_upper)))\n",
    "with open('distant_supervision/countries.dic') as countries_dic:\n",
    "    for line in countries_dic:\n",
    "        duck_dict.add(line.split('\\n')[0])\n",
    "        duck_dict_first_char_upper.add(line.split('\\n')[0])\n",
    "with open('distant_supervision/metric_prefix.dic') as prefix_dic:\n",
    "    for line in prefix_dic:\n",
    "        duck_dict.add(line.split('\\n')[0])\n",
    "        duck_dict_first_char_upper.add(line.split('\\n')[0])\n",
    "print(\"With explicit included words Duck dict contains now \" + str(len(duck_dict)) + \" English words.\")\n",
    "\n",
    "spell_right_dict = set()\n",
    "spell_right_dict_lower = set()\n",
    "with open('distant_supervision/spellright.dic') as eng_dic:\n",
    "    for line in eng_dic:\n",
    "        if '/' in line:\n",
    "            spell_right_dict.add(line.split('/')[0])\n",
    "            spell_right_dict_lower.add(line.split('/')[0].lower())\n",
    "        else:\n",
    "            spell_right_dict.add(line.split('\\n')[0])\n",
    "            spell_right_dict_lower.add(line.split('\\n')[0].lower())\n",
    "print(\"Spellright dict contained \" + str(len(spell_right_dict)) + \" English words.\")\n",
    "\n",
    "acronym_dic = set()\n",
    "with open('distant_supervision/acronyms.dic') as acro_dic:\n",
    "    for line in acro_dic:\n",
    "        acronym_dic.add(line.split('\\n')[0])\n",
    "print(\"Loaded a set of \" + str(len(acronym_dic)) + \" Acronyms.\")\n",
    "\n",
    "alpha = ['T', 'C', 'G', 'A']\n",
    "gen_seq_triplets = [''.join(seq) for seq in product(alpha, repeat = 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just a simple save so that we can easily get the values from other python scripts or notebooks in the same enviornment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store known_software_01_09\n",
    "%store duck_dict_first_char_upper\n",
    "%store known_software_lower_01_09\n",
    "%store duck_dict_lower\n",
    "%store acronym_dic\n",
    "%store gen_seq_triplets\n",
    "\n",
    "import pickle\n",
    "pickle.dump(known_software_01_09, open(\"known_software_01_09.p\", \"wb\"))\n",
    "pickle.dump(duck_dict_first_char_upper, open(\"duck_dict_first_char_upper.p\", \"wb\"))\n",
    "pickle.dump(known_software_lower_01_09, open(\"known_software_lower_01_09.p\", \"wb\"))\n",
    "pickle.dump(duck_dict_lower, open(\"duck_dict_lower.p\", \"wb\"))\n",
    "pickle.dump(acronym_dic, open(\"acronym_dic.p\", \"wb\"))\n",
    "pickle.dump(gen_seq_triplets, open(\"gen_seq_triplets.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Function\n",
    "\n",
    "- WikiData is the basis for distant supervision\n",
    "- But the exhaustive list matches to much: include a dictionary of plain english words which will serve to exlude WikiData matches\n",
    "- Duck et al. Dictionary is actually better than a free web resource. Both in number of tp and fp. \n",
    "- Recall numbers are actually quite good \n",
    "- Precision is still not satisfactory\n",
    "- Next consideration: Also include a dictionary of abbreviations to include? (of course not software abbreviations) -> not helpful\n",
    "- Do partial matches help? -> Increase run time way to much to be helpful\n",
    "\n",
    "Observations:\n",
    "- Some false positives are caused through 'normal' words which stand on the beginning of the sentence, e.g. Random, Motion, because the dictionary only contains them lowercased. A possible solution could be to include first letter upper case to the dictionary: without this we have a recognition of tp 274, fp 774 and fn 135. With it we have: 240, 486, 169. This trade of could definitely be considered worth it.\n",
    "- A better tokenization would actually help a lot. A large source of false positive is splitting of tokens on hypentation. The question is how to resolve this: best would be in spacy since it is so nicely integrated in snorkel. It is possible to create custom rules from an existing Spacy parser (following this post https://support.prodi.gy/t/how-to-tell-spacy-not-to-split-any-intra-hyphen-words/1456/4 and the docs https://spacy.io/usage/linguistic-features#native-tokenizer-additions). Tokenization had to be adjusted INSIDE THE SNORKEL CODE WE BUILT FROM SOURCE.\n",
    "- A single small r is matched. This is a problem and is happening because of the lowercased software names. One solution could be to only lower case names which are longer than 2 or 3 characters, this would probably allow to exclude abbreviations which should not be lowercased and are probably mostly used correctly. \n",
    "- 'Switzerland' is in software names. This is strange but could be easily fixed by adding country names explicitly to the dictionary (which does not contain them right now).  \n",
    "- A lot of false positives acutally come from mentioning of conventional software, where the developers name matches the software name and is mentioned a second time in brackets. This is impossible to exlude with distant supervision alone. \n",
    "- 'Review Board' (which is an actual software) is a source of a lot of false positives. This could maybe be addressed by matching 'Institutional' or 'Institute' before the actual word, becaues in that case it does commonly not refer to the software (the rule is applied in general not just on a specific target, either way this might boarder on a overspecificiation). \n",
    "- Consider lower cased software names? This could be problematic or helping: With considering them we have 240, 444, 169. Without considering them we have . Maybe do not lowercase all candidates?? Exlude those which are exlusively upper case or have more than one upper case letters? This does actually undermine the purpose of the initial lower match, but could also find to help a trade of for its application. \n",
    "- Maybe exclude words that are directly in brackets? Include **all types of brackets**. (Maybe also just a left bracket, followed by commas and closing in at least 6 tokens?? All of those points need individual testing. --- **do not apply**). \n",
    "- Explicitly adding metric prefixes which are sometimes mentioned in text and ambiguous with software.. add them to dictionary explicitly\n",
    "- A lot of errors that remain are acutally reocurring specific words: 'GPs', 'Tween', 'California', 'Geneva', 'ART' (this actually also appears as true positives), 'actin', 'PubMed', 'R' (in the wrong context), 'control groups', 'NaCl', 'ELISA'\n",
    "- **.get_span()** method DOES **NOT PERFORM AS EXPECTED**: does not acutally return the entire span! Quickfixing this brings 50 less false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Distance learning function\n",
    "def LF_distant_supervision(c, software_dict, software_dict_lower, english_dict, english_dict_lower, acronym_dict, gen_seqs):\n",
    "    cand = c[0].get_span()\n",
    "    tokens = [x.lower() for x in c[0].get_attrib_tokens()]\n",
    "    if len(tokens) == 1 and len(tokens[0]) != len(cand):\n",
    "        return 0\n",
    "    omissions = ['California', 'NaCl', 'control groups', 'FID', 'ELISA', 'GPs', 'PubMed', 'Gaussian', 'synaptic', 'vivo', 'ionic']\n",
    "    if cand in omissions:\n",
    "        return -1\n",
    "    if len(cand) == 2 or cand.isdigit() or all(char in string.punctuation for char in cand):\n",
    "        return -1\n",
    "    cand_lower = cand.lower()\n",
    "    cand_in_known_software = cand in software_dict\n",
    "    cand_in_english_dic = cand in english_dict # english_dict\n",
    "    cand_lower_match_known_software = cand_lower in software_dict_lower\n",
    "    cand_lower_match_english_dic = cand_lower in english_dict_lower # english_dict_lower\n",
    "    cand_is_acronym = cand in acronym_dict\n",
    "    cand_is_gen_seq = cand in gen_seqs\n",
    "    \n",
    "    left_tokens = [x for x in get_left_tokens(c, window=1)]\n",
    "    right_tokens = [x for x in get_right_tokens(c, window=1)]\n",
    "\n",
    "    if ('institutional' in left_tokens or \n",
    "        'institution' in left_tokens or \n",
    "        'ethics' in left_tokens or \n",
    "        'ethic' in left_tokens or \n",
    "        (len(left_tokens) > 0 and len(right_tokens) > 0 and left_tokens[-1] in ['(', '[', '{'] and right_tokens[0] in [')', ']', '}'])):\n",
    "        return -1\n",
    "    \n",
    "    if cand_is_gen_seq:\n",
    "        return -1\n",
    "    if cand_in_english_dic:\n",
    "        if cand_in_known_software:\n",
    "            return 0#-1\n",
    "        else: \n",
    "            return -1\n",
    "    else:\n",
    "        if cand_in_known_software:\n",
    "            return 1\n",
    "        elif cand_lower_match_known_software:\n",
    "            return 1\n",
    "        elif cand_lower_match_english_dic:\n",
    "            return 0 # -1 \n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LF_dist = partial(LF_distant_supervision, \n",
    "                  software_dict=known_software_01_09,\n",
    "                  software_dict_lower=known_software_lower_01_09,\n",
    "                  english_dict=duck_dict_first_char_upper,\n",
    "                  english_dict_lower=duck_dict_lower,\n",
    "                  acronym_dict=acronym_dic,\n",
    "                  gen_seqs=gen_seq_triplets)\n",
    "\n",
    "lf = LF_dist\n",
    "test_marginals  = np.array([0.5 * (lf(c) + 1) for c in test_cands])\n",
    "tp, fp, tn, fn = scorer.score(test_marginals, set_unlabeled_as_neg=True, set_at_thresh_as_neg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SentenceNgramViewer(fp, session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
