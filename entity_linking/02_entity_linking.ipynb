{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Linking\n",
    "\n",
    "As a result of our knowledge extraction from scientific articles, we know now which software is present in articles. However, the problem remains, that authors name the same software differently. Therefore, in order to allow for the best reasoning we need to be able to map different names to the same distinct software. This is implemented here.\n",
    "\n",
    "First we need to import the list we generated in the previous steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "software_counter = []\n",
    "with open('software_reasoning_list_total.csv', 'r') as software_file:\n",
    "    software_csv = csv.reader(software_file)\n",
    "    for idx, row in enumerate(software_csv):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        software_counter.append({\n",
    "            'names': [row[0]],\n",
    "            'count': int(row[1]),\n",
    "            'unique name': ''\n",
    "        })\n",
    "software_counter_bak = software_counter.copy()\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stops =  stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_software_counts = {}\n",
    "with open('software_reasoning_list_total.csv', 'r') as software_file:\n",
    "    software_csv = csv.reader(software_file)\n",
    "    for idx, row in enumerate(software_csv):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        base_software_counts[row[0]] = int(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start out with considering simple spelling deviations: casing, special characters and numbers. \n",
    "Each time we find a match we will combine the buckets in the list by appending their names. \n",
    "How the unique name is assigned will be considered later on.\n",
    "\n",
    "We can also remove stopwords and try to stem 'normal' words appearing in software names. The stemming needs to be smart enough to perform wrong stemming ob abbreviations or other 'unstemmable' words.  \n",
    "\n",
    "We can also remove errors at this point. For example if only a single token was extracted that is a special character. In this case our transformation pipeline will just give an empty example. In this case we know that we have found an error. \n",
    "\n",
    "We also take into account abbreviations which we build from the first letters and use to improve the linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "software_counter = software_counter_bak.copy()\n",
    "\n",
    "def match_buckets(software_counter):\n",
    "    capitalizer = lambda x: x.casefold()\n",
    "    normalize_string = lambda x: re.sub('[^0-9a-zA-Z]+', ' ', x)\n",
    "    remove_number = lambda x: x.rstrip('0123456789 ,.').lstrip(' ')\n",
    "    remove_pro = lambda x: x[:-4] if x.endswith(' pro') else x\n",
    "    stemming = lambda x: ' '.join([stemmer.stem(a) for a in x.split()]) if len(x.split()) > 1 else x\n",
    "    rm_stopwords = lambda x: ' '.join([w for w in x.split() if w not in stops])\n",
    "    get_acronym = lambda x: ''.join([s[0] for s in x.split()]) if len(x.split()) > 2 else None\n",
    "\n",
    "    indices_to_pop = list()\n",
    "    for cur_idx, cur_software in enumerate(software_counter):\n",
    "        cur_names = list(map(capitalizer, cur_software['names']))\n",
    "        cur_names = list(map(normalize_string, cur_names))\n",
    "        cur_names = list(map(remove_number, cur_names))\n",
    "        cur_names = list(map(remove_pro, cur_names))\n",
    "        cur_names = list(map(rm_stopwords, cur_names))\n",
    "        cur_acronyms = set(map(get_acronym, cur_names))   \n",
    "        cur_names = list(map(stemming, cur_names))\n",
    "        for next_idx in range(cur_idx + 1, len(software_counter)):\n",
    "            next_names = list(map(capitalizer, software_counter[next_idx]['names']))\n",
    "            next_names = list(map(normalize_string, next_names))\n",
    "            next_names = list(map(remove_number, next_names))\n",
    "            next_names = list(map(rm_stopwords, next_names))\n",
    "            next_acronyms = set(map(get_acronym, next_names))\n",
    "            next_names = list(map(stemming, next_names))\n",
    "            if any(s_name in next_names for s_name in cur_names)\\\n",
    "                or any(acro in next_names for acro in cur_acronyms)\\\n",
    "                or any(acro in cur_names for acro in next_acronyms):\n",
    "                indices_to_pop.append(cur_idx)\n",
    "                for n in cur_software['names']:\n",
    "                    software_counter[next_idx]['names'].append(n)\n",
    "                software_counter[next_idx]['count'] += cur_software['count']\n",
    "                break\n",
    "    if indices_to_pop:\n",
    "        indices_to_pop.reverse()\n",
    "        for idx in indices_to_pop:\n",
    "            software_counter.pop(idx)\n",
    "    \n",
    "    return software_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_buckets(software_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Linking with DBpedia\n",
    "\n",
    "We created buckets of names which belong to the same entity. \n",
    "Next we map them to the names found in DBpedia and further match the buckets.\n",
    "We iterate over the buckets and if a **single** name matches a DBpedia entry we map it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_names = pd.read_csv('dbpedia_software_long.csv.gz', compression='gzip')\n",
    "unique_db_labels = set(dbpedia_names['unique'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_unique_name(software_list):\n",
    "    direct_match = [x in unique_db_labels for x in software_list]\n",
    "    matched_names = [x for x,y in zip(software_list, direct_match) if y]\n",
    "    #print(direct_match)\n",
    "    #alt_name_match =\n",
    "    if any(direct_match):\n",
    "        if len(set(matched_names)) > 1:\n",
    "            print(\"Found multiple matches in direct match:\")\n",
    "            print(software_list)\n",
    "            print(matched_names)\n",
    "            return []\n",
    "        else:\n",
    "            return matched_names[0]\n",
    "    else: \n",
    "        alt_name_match = dbpedia_names.loc[dbpedia_names['label'].isin(software_list)]\n",
    "        if len(alt_name_match.index) > 0:\n",
    "            unique_name_list = alt_name_match['unique'].tolist()\n",
    "            if len(set(unique_name_list)) > 1:\n",
    "                print(\"Found multiple matches in indirect match:\")\n",
    "                print(software_list)\n",
    "                print(unique_name_list)\n",
    "                return []\n",
    "            else:\n",
    "                return unique_name_list[0]\n",
    "        else:\n",
    "            for idx, row in dbpedia_names.iterrows():\n",
    "                developer = row['developer']\n",
    "                software_name = row['unique']\n",
    "                software_label = row['label']\n",
    "                match_list = []\n",
    "                for software in software_list:\n",
    "                    if not pd.isna(developer) and not pd.isna(software_name) and not pd.isna(software_label) and developer in software and (software_name in software or software_label in software):\n",
    "                        match_list.append(True)\n",
    "                        return software_name\n",
    "                    else:\n",
    "                        match_list.append(False)\n",
    "                \n",
    "                \n",
    "    return []\n",
    "\n",
    "def get_unique_names_multi(software_list):\n",
    "    unique_name = get_unique_name(software_list['names'])\n",
    "    return unique_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = Pool(processes=24)\n",
    "unique_names = p.map(get_unique_names_multi, software_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump( unique_names, open( \"unique_names_backup.p\", \"wb\" ) )\n",
    "#unique_names = pickle.load(open( \"unique_names_backup.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_unique_names(software_buckets, mapped_names):\n",
    "    software_mapping = {}\n",
    "    distinct_mapped_count = 0\n",
    "    names_mapped_count = 0\n",
    "    total_mapped_count = 0\n",
    "    distinct_not_mapped_count = 0\n",
    "    names_not_mapped_count = 0\n",
    "    total_not_mapped_count = 0\n",
    "    for s, n in zip(software_buckets, mapped_names):\n",
    "        if n:\n",
    "            distinct_mapped_count += 1\n",
    "            names_mapped_count += len(s['names'])\n",
    "            total_mapped_count += s['count']\n",
    "            if n in software_mapping.keys():\n",
    "                # append to existing \n",
    "                software_mapping[n]['alias'].extend(s['names'])\n",
    "                software_mapping[n]['count'] += s['count']\n",
    "            else:\n",
    "                # create new\n",
    "                software_mapping[n] = {\n",
    "                    'alias': s['names'],\n",
    "                    'count': s['count']\n",
    "                }\n",
    "        else:\n",
    "            distinct_not_mapped_count += 1\n",
    "            names_not_mapped_count += len(s['names'])\n",
    "            total_not_mapped_count += s['count']\n",
    "            # create new, choose max occurrence in upper case as unique name \n",
    "            max_count = -1\n",
    "            chosen_unique = ''\n",
    "            for name in s['names']:\n",
    "                if base_software_counts[name] > max_count:\n",
    "                    max_count = base_software_counts[name]\n",
    "                    chosen_unique = name\n",
    "            chosen_unique = chosen_unique\n",
    "            software_mapping[chosen_unique] = {\n",
    "                'alias': s['names'],\n",
    "                'count': s['count']\n",
    "            }\n",
    "    return software_mapping, distinct_mapped_count, names_mapped_count, total_mapped_count, distinct_not_mapped_count, names_not_mapped_count, total_not_mapped_count\n",
    "\n",
    "final_mapping, p_count, n_mapped_count, tp_count, n_count, n_not_mapped_count, tn_count = combine_unique_names(software_counter, unique_names)\n",
    "print(\"Mapped {} distinct positives to DBpedia amounting to {} total positives. {} distincs ({} total) were not mapped\". format(p_count, tp_count, n_count, tn_count))\n",
    "print(\"{} and {} names were mapped and not mapped\".format(n_mapped_count, n_not_mapped_count))\n",
    "print(\"Reduced number of buckets from {} to {}\".format(len(base_software_counts), len(final_mapping)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( final_mapping, open( \"final_software_mapping.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the final state of the linking.  \n",
    "The outputs are now written to a file in which we plan to gather more information on the software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_reasoning_list = pd.read_csv('software_reasoning_list_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = []\n",
    "example_column = []\n",
    "with open(\"data/software_kg_with_pos_pre_s_3_gold_dev_d_04_l_00015_s_01_3.json\", \"r\") as software_kg:\n",
    "    kg = json.load(software_kg)\n",
    "    for s_name in software_reasoning_list['name']:\n",
    "        found_article = False\n",
    "        for article in kg['@graph']:\n",
    "            for software in article['https://data.gesis.org/softwarekg/software']:\n",
    "                if software['https://schema.org/name'] == s_name:\n",
    "                    # get article doi as example\n",
    "                    doi = article['http://schema.org/identifier']\n",
    "                    found_article = True\n",
    "                    break\n",
    "            if found_article = True:\n",
    "                break\n",
    "        name_found = False\n",
    "        for unique_name in final_mapping:\n",
    "            if s_name in final_mapping[unique_name]['alias']:\n",
    "                new_column.append(unique_name)       \n",
    "                name_found = True\n",
    "                break\n",
    "        if not name_found:\n",
    "            print(\"This case points toward some error in the original data frame: name {} is not in the software list\".format(s_name))\n",
    "            new_column.append('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_reasoning_list['linked_name'] = new_column\n",
    "\n",
    "cols = software_reasoning_list.columns.tolist()\n",
    "\n",
    "cols = cols[0:2] + cols[-1:] + cols[2:-1]\n",
    "software_reasoning_list = software_reasoning_list[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_reasoning_list.to_csv(\"software_reasoning_list_linked_names.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an example article for each software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missed_counter = 0\n",
    "linked_software_list = {}\n",
    "for idx, bucket in enumerate(software_counter):\n",
    "    if not any([\"IBM\" in s for s in bucket['names']]):\n",
    "        continue\n",
    "    print(bucket['names'])\n",
    "    if idx % 100 == 0:\n",
    "        print(\"Processed {}\".format(idx))\n",
    "    unique_name = get_unique_name(bucket['names'])\n",
    "    if unique_name and unique_name in linked_software_list.keys():\n",
    "        linked_software_list[unique_name]['names'].extend(bucket['names'])\n",
    "        linked_software_list[unique_name]['count'] += bucket['count']\n",
    "    elif unique_name: \n",
    "        linked_software_list[unique_name] = {\n",
    "            'names': bucket['names'],\n",
    "            'count': bucket['count']\n",
    "        }\n",
    "    else:\n",
    "        missed_counter += 1\n",
    "\n",
    "print(linked_software_list)\n",
    "print(\"Missed {}\".format(missed_counter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
