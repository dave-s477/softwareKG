{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Reasoning Data\n",
    "\n",
    "Combine the information gathered through the neural network and construct the knowledge graph. \n",
    "\n",
    "Extract some statistics of the resulting data and generate the setup for manual information enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Information from Gold Data\n",
    "\n",
    "Test the knowledge graph construction on the hand annotated SoSci gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "from os import listdir, mkdir\n",
    "from os.path import join, exists\n",
    "from util.doc_info import get_doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('data/sosci_reasoning'):\n",
    "    mkdir('data/sosci_reasoning')\n",
    "    \n",
    "with open('data/sosci_bio.txt', 'r') as bio_file:\n",
    "    line = bio_file.readline()\n",
    "    current_doc = ''\n",
    "    current_candidate = ''\n",
    "    current_candidates = []\n",
    "    while line:\n",
    "        if line in ['\\n', '\\t\\n']:\n",
    "            if current_candidate:\n",
    "                current_candidates.append(current_candidate)\n",
    "            current_candidate = ''\n",
    "        elif line.startswith('-DOCSTART-'):\n",
    "            if current_candidate:\n",
    "                current_candidates.append(current_candidate)\n",
    "            current_candidate = ''\n",
    "            # Here we write a document if we already have one. \n",
    "            if current_doc:\n",
    "                article_info = get_doc_dict(current_doc, current_candidates)\n",
    "                #json.dump(document_dict, json_file, indent=4)\n",
    "                with open('data/sosci_reasoning/'+current_doc+'.json', 'w') as out_file:\n",
    "                    json.dump(article_info, out_file, indent=4)\n",
    "                #print(article_info)\n",
    "            current_candidates = []\n",
    "            current_doc = line.split(':')[1].rstrip('\\n')\n",
    "        else:\n",
    "            token, annotation = line.split('\\t')\n",
    "            annotation = annotation.rstrip('\\n')\n",
    "            if current_candidate:\n",
    "                if annotation == 'O':\n",
    "                    current_candidates.append(current_candidate)\n",
    "                    current_candidate = ''\n",
    "                elif annotation == 'B-software':\n",
    "                    current_candidates.append(current_candidate)\n",
    "                    current_candidate = token\n",
    "                elif annotation == 'I-software':\n",
    "                    current_candidate += ' {}'.format(token) # extend\n",
    "            else:\n",
    "                if annotation == 'O':\n",
    "                    pass\n",
    "                elif annotation == 'B-software':\n",
    "                    current_candidate = token\n",
    "                elif annotation == 'I-software':\n",
    "                    print(\"This is not allowed to happen.\")\n",
    "        line = bio_file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we combine the information from all separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_kg = {\n",
    "  \"@context\": {\n",
    "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "    \"base\": \"http://data.gesis.org/softwarekg/\",\n",
    "    \"schema\": \"http://schema.org/\",\n",
    "    \"swo\": \"http://www.ebi.ac.uk/swo/swo.owl#\",\n",
    "    \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "    \"dct\": \"http://purl.org/dc/elements/1.1/\",\n",
    "    \"nif\": \"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\"\n",
    "    }\n",
    "}\n",
    "graph = []\n",
    "for file in listdir('data/sosci_reasoning'):\n",
    "    with open('data/sosci_reasoning/'+file, 'r') as json_file:\n",
    "        graph_entry = json.load(json_file)\n",
    "        graph.append(graph_entry)\n",
    "software_kg['@graph'] = graph\n",
    "with open('data/software_kg_sosci.json', 'w') as kg:\n",
    "    json.dump(software_kg, kg, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the plain software names in order to analyze how often they appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "with open('data/software_kg_sosci.json', 'r') as json_file:\n",
    "    kg = json.load(json_file)\n",
    "    for article_node in kg['@graph']:\n",
    "        for software_node in article_node['http://data.gesis.org/softwarekg/software']:\n",
    "            software_name = software_node['http://schema.org/name']\n",
    "            c[software_name] += 1\n",
    "\n",
    "with open('data/software_counted_list_sosci.csv', 'w') as csv_file:\n",
    "    fieldnames = ['name', 'count']\n",
    "    software_writer = csv.writer(csv_file, delimiter=',', quotechar='\"')\n",
    "    software_writer.writerow(fieldnames)\n",
    "    for s in c.most_common():\n",
    "        software_writer.writerow(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Information from the Silver Standard\n",
    "\n",
    "We can of course look at the same information based on the silver standard. However, they are not as interesting because they are only suggestively labeled. \n",
    "Therefore, the construction of a knowledge graph is also less intersting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silver_standard_data = 'data/pos_silver_samples_cor_data.txt'\n",
    "silver_standard_labels = 'data/pos_silver_samples_cor_labels.txt'\n",
    "if not exists('data/silver_standard_reasoning'):\n",
    "    mkdir('data/silver_standard_reasoning')\n",
    "    \n",
    "c = Counter()\n",
    "error_count = 0\n",
    "with open(silver_standard_data, 'r') as data_file, open(silver_standard_labels, 'r') as labels_file:\n",
    "    data_line = data_file.readline()\n",
    "    labels_line = labels_file.readline()\n",
    "    current_candidate = ''\n",
    "    current_candidates = []\n",
    "    counter = 0\n",
    "    while data_line and labels_line:\n",
    "        counter += 1\n",
    "        tokens = data_line.split()\n",
    "        labels = labels_line.split()\n",
    "        token = tokens.pop(0)\n",
    "        annotation = labels.pop(0)\n",
    "        while token and annotation:\n",
    "            if current_candidate:\n",
    "                if annotation == 'O':\n",
    "                    c[current_candidate] += 1\n",
    "                    current_candidate = ''\n",
    "                elif annotation == 'B-software':\n",
    "                    c[current_candidate] += 1\n",
    "                    current_candidate = token\n",
    "                elif annotation == 'I-software':\n",
    "                    current_candidate += ' {}'.format(token) # extend\n",
    "            else:\n",
    "                if annotation == 'O':\n",
    "                    pass\n",
    "                elif annotation == 'B-software':\n",
    "                    current_candidate = token\n",
    "                elif annotation == 'I-software':\n",
    "                    error_count += 1\n",
    "                    #print(\"This is not supposed to happen.\")\n",
    "                    #print(data_line)\n",
    "                    #print(labels_line)\n",
    "                    #print(token)\n",
    "                    #print('#####\\n')\n",
    "            if len(tokens) > 0 and len(labels) > 0:\n",
    "                token = tokens.pop(0)\n",
    "                annotation = labels.pop(0)\n",
    "            else:\n",
    "                token = None\n",
    "                annotation = None\n",
    "        if current_candidate:\n",
    "            c[current_candidate] += 1\n",
    "        current_candidate = ''\n",
    "                \n",
    "        data_line = data_file.readline()\n",
    "        labels_line = labels_file.readline()\n",
    "        #if counter > 10000:\n",
    "        #    break\n",
    "print(\"Errors: {}\".format(error_count))\n",
    "\n",
    "with open('data/software_counted_list_silver_standard.csv', 'w') as csv_file:\n",
    "    fieldnames = ['name', 'count']\n",
    "    software_writer = csv.writer(csv_file, delimiter=',', quotechar='\"')\n",
    "    software_writer.writerow(fieldnames)\n",
    "    for s in c.most_common():\n",
    "        software_writer.writerow(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information from Reasoning Data\n",
    "\n",
    "Large scale information extraction from all data in our reasoning set. \n",
    "Here we combine all reasoning outputs our model has created in a knowledge graph an extract the first statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_location = 'data/reasoning_output_production_model/'\n",
    "reasoning_files = listdir(reasoning_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_total = Counter()\n",
    "c_relative = Counter()\n",
    "empty_file_num = 0\n",
    "for file in reasoning_files:\n",
    "    #print(file)\n",
    "    try:\n",
    "        with open(join(reasoning_location, file), 'r') as json_file:\n",
    "            software_in_article = []\n",
    "            data = json.load(json_file)\n",
    "            for software in data['http://data.gesis.org/softwarekg/software']:\n",
    "                software_in_article.append(software['http://schema.org/name'])\n",
    "            c_total.update(software_in_article)\n",
    "            c_relative.update(set(software_in_article))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Empty file {}\".format(file))\n",
    "        empty_file_num += 1\n",
    "print(\"Totally extracted software: {}\".format(sum(c_total.values())))\n",
    "print(\"Relative per paper extracted software: {}\".format(sum(c_relative.values())))\n",
    "print(\"{} empty files in total\".format(empty_file_num))\n",
    "with open('software_reasoning_production_model.csv', 'w') as total_file, open('software_reasoning_production_model_relative.csv', 'w') as relative_file:\n",
    "    fieldnames = ['name', 'count']\n",
    "    software_writer_total = csv.writer(total_file, delimiter=',', quotechar='\"')\n",
    "    software_writer_total.writerow(fieldnames)\n",
    "    for s in c_total.most_common():\n",
    "        software_writer_total.writerow(s)\n",
    "    software_writer_relative = csv.writer(relative_file, delimiter=',', quotechar='\"')\n",
    "    software_writer_relative.writerow(fieldnames)\n",
    "    for s in c_relative.most_common():\n",
    "        software_writer_relative.writerow(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a error in the following code itis likely that a empty file was generated during prediction. \n",
    "To not overlook errors we manually check how much and why that happend `find path_to_reasoning_files -empty (-delete)`. \n",
    "In the current final run 3 files came up empty and were ignored:\n",
    "\n",
    "./10.1371_journal.pone.0069554.json\n",
    "\n",
    "./10.1371_journal.pone.0069504.json\n",
    "\n",
    "./10.1371_journal.pmed.1001418.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_kg = {\n",
    "  \"@context\": {\n",
    "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "    \"base\": \"http://data.gesis.org/softwarekg/\",\n",
    "    \"schema\": \"http://schema.org/\",\n",
    "    \"swo\": \"http://www.ebi.ac.uk/swo/swo.owl#\",\n",
    "    \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "    \"dct\": \"http://purl.org/dc/elements/1.1/\",\n",
    "    \"nif\": \"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\"\n",
    "    }\n",
    "}\n",
    "graph = []\n",
    "for file in listdir('data/reasoning_output_production_model'):\n",
    "    with open('data/reasoning_output_production_model/'+file, 'r') as json_file:\n",
    "        graph_entry = json.load(json_file)\n",
    "        graph.append(graph_entry)\n",
    "software_kg['@graph'] = graph\n",
    "with open('data/software_kg_production_model.json', 'w') as kg:\n",
    "    json.dump(software_kg, kg, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_nightly)",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
