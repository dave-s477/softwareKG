{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Standard handling\n",
    "Correct the silver standard and split it into smaller chunks to efficiently perform pretraining on it. \n",
    "\n",
    "When exporting the silver standard data some labeling irregularities where inserted. Those can easily be fixed automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_standard_data = '../data/data/pos_silver_samples_cor_data.txt'\n",
    "silver_standard_labels = '../data/pos_silver_samples_cor_labels.txt'\n",
    "silver_standard_labels_cor = '../data/pos_silver_samples_corrected_labels.txt'\n",
    "    \n",
    "error_count = 0\n",
    "with open(silver_standard_data, 'r') as data_file, open(silver_standard_labels, 'r') as labels_file, open(silver_standard_labels_cor, 'w') as labels_file_cor:\n",
    "    data_line = data_file.readline()\n",
    "    labels_line = labels_file.readline()\n",
    "    current_candidate = ''\n",
    "    counter = 0\n",
    "    while data_line and labels_line:\n",
    "        counter += 1\n",
    "        output_labels = ''\n",
    "        tokens = data_line.split()\n",
    "        labels = labels_line.split()\n",
    "        token = tokens.pop(0)\n",
    "        annotation = labels.pop(0)\n",
    "        i_allowed_to_occur = False\n",
    "        print_flag = False\n",
    "        while token and annotation:\n",
    "            if i_allowed_to_occur:\n",
    "                if annotation == 'O':\n",
    "                    i_allowed_to_occur = False\n",
    "                    output_labels += ' O'\n",
    "                elif annotation == 'B-software':\n",
    "                    i_allowed_to_occur = True\n",
    "                    output_labels += ' B-software'\n",
    "                elif annotation == 'I-software':\n",
    "                    i_allowed_to_occur = True\n",
    "                    output_labels += ' I-software'\n",
    "            else:\n",
    "                if annotation == 'O':\n",
    "                    i_allowed_to_occur = False\n",
    "                    output_labels += ' O'\n",
    "                elif annotation == 'B-software':\n",
    "                    i_allowed_to_occur = True\n",
    "                    output_labels += ' B-software'\n",
    "                elif annotation == 'I-software':\n",
    "                    i_allowed_to_occur = False\n",
    "                    output_labels += ' O'\n",
    "                    print_flag = True\n",
    "            if len(tokens) > 0 and len(labels) > 0:\n",
    "                token = tokens.pop(0)\n",
    "                annotation = labels.pop(0)\n",
    "            else:\n",
    "                token = None\n",
    "                annotation = None\n",
    "        output_labels += '\\n'\n",
    "        if print_flag:\n",
    "            print(\"Replace\")\n",
    "            print(labels_line)\n",
    "            print(output_labels)\n",
    "            print(len(data_line.split()))\n",
    "            print(len(output_labels.split()))\n",
    "                \n",
    "        data_line = data_file.readline()\n",
    "        labels_line = labels_file.readline()\n",
    "        labels_file_cor.write(output_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we hav to look up how much silver data is overall available. \n",
    "Since the dataset is extremely large its not feasible to train on all articles for each epoch. \n",
    "Instead we train one epoch on all positive samples and randomly sampled negative samples until we saw all negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = {\n",
    "    '../data/pos_silver_samples_cor_data.txt': [],\n",
    "    '../data/pos_silver_samples_cor_labels.txt': [],\n",
    "    '../data/neg_silver_samples_data.txt': []\n",
    "}\n",
    "for dataset in buckets.keys():\n",
    "    with open(dataset, 'r') as data:\n",
    "        for line in data:\n",
    "            buckets[dataset].append(line)\n",
    "\n",
    "for dataset in buckets.keys():\n",
    "    print(len(buckets[dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the size of our sets we decided to split it into 12 sets, where one positive sample is always accompanied by one negative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "NUM_SILVER_TRAIN = 12\n",
    "\n",
    "for i in range(NUM_SILVER_TRAIN):\n",
    "    print(\"Creating set {}\".format(i))\n",
    "    with open('data/merged_silver_standard_data_ep{}.txt'.format(i), 'w') as merged_d, open('data/merged_silver_standard_labels_ep{}.txt'.format(i), 'w') as merged_l:\n",
    "        for idx, (x, y) in enumerate(zip(buckets['data/pos_silver_samples_cor_data.txt'], buckets['data/pos_silver_samples_cor_labels.txt'])):\n",
    "            if idx % 50000 == 0:\n",
    "                print(\"At index {}\".format(idx))\n",
    "            merged_d.write(x)\n",
    "            merged_l.write(y)\n",
    "            index_to_pop = random.randint(0, len(buckets['data/neg_silver_samples_data.txt'])-1)\n",
    "            text_string = buckets['data/neg_silver_samples_data.txt'].pop(index_to_pop)\n",
    "            merged_d.write(text_string)\n",
    "            labels_string = \" \"\n",
    "            for a in range(len(text_string.split())):\n",
    "                labels_string += \"O \"\n",
    "            labels_string += '\\n'\n",
    "            merged_l.write(labels_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(buckets['data/neg_silver_samples_data.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a split for optimizing the pre-training\n",
    "\n",
    "This split is intended to just test how well the model is able to learn on the silver standard and to predict on the silver standard. \n",
    "For this purpose the silver standard is split into a train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "size_of_silver_set = 25000\n",
    "\n",
    "print(\"Creating the silver devel set\")\n",
    "with open('data/merged_silver_opt_test_data.txt', 'w') as merged_d, open('data/merged_silver_opt_test_labels.txt', 'w') as merged_l:\n",
    "    for idx in range(size_of_silver_set):\n",
    "        if idx % 50000 == 0:\n",
    "            print(\"At index {}\".format(idx))\n",
    "        pos_index_to_pop = random.randint(0, len(buckets['data/pos_silver_samples_cor_data.txt'])-1)\n",
    "        neg_index_to_pop = random.randint(0, len(buckets['data/neg_silver_samples_data.txt'])-1)\n",
    "        pos_text_string = buckets['data/pos_silver_samples_cor_data.txt'].pop(pos_index_to_pop)\n",
    "        pos_text_label = buckets['data/pos_silver_samples_cor_labels.txt'].pop(pos_index_to_pop)\n",
    "        merged_d.write(pos_text_string)\n",
    "        merged_l.write(pos_text_label)\n",
    "        neg_text_string = buckets['data/neg_silver_samples_data.txt'].pop(neg_index_to_pop)\n",
    "        merged_d.write(neg_text_string)\n",
    "        labels_string = \" \"\n",
    "        for a in range(len(neg_text_string.split())):\n",
    "            labels_string += \"O \"\n",
    "        labels_string += '\\n'\n",
    "        merged_l.write(labels_string)\n",
    "\n",
    "NUM_SILVER_TRAIN = 14\n",
    "\n",
    "for i in range(NUM_SILVER_TRAIN):\n",
    "    print(\"Creating set {}\".format(i))\n",
    "    with open('data/merged_silver_opt_train_data_ep{}.txt'.format(i), 'w') as merged_d, open('data/merged_silver_opt_train_labels_ep{}.txt'.format(i), 'w') as merged_l:\n",
    "        for idx, (x, y) in enumerate(zip(buckets['data/pos_silver_samples_cor_data.txt'], buckets['data/pos_silver_samples_cor_labels.txt'])):\n",
    "            if idx % 50000 == 0:\n",
    "                print(\"At index {}\".format(idx))\n",
    "            merged_d.write(x)\n",
    "            merged_l.write(y)\n",
    "            index_to_pop = random.randint(0, len(buckets['data/neg_silver_samples_data.txt'])-1)\n",
    "            text_string = buckets['data/neg_silver_samples_data.txt'].pop(index_to_pop)\n",
    "            merged_d.write(text_string)\n",
    "            labels_string = \" \"\n",
    "            for a in range(len(text_string.split())):\n",
    "                labels_string += \"O \"\n",
    "            labels_string += '\\n'\n",
    "            merged_l.write(labels_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(12,14):\n",
    "    print(\"Creating set {}\".format(i))\n",
    "    with open('data/merged_silver_opt_train_data_ep{}.txt'.format(i), 'w') as merged_d, open('data/merged_silver_opt_train_labels_ep{}.txt'.format(i), 'w') as merged_l:\n",
    "        for idx, (x, y) in enumerate(zip(buckets['data/pos_silver_samples_cor_data.txt'], buckets['data/pos_silver_samples_cor_labels.txt'])):\n",
    "            if idx % 50000 == 0:\n",
    "                print(\"At index {}\".format(idx))\n",
    "            merged_d.write(x)\n",
    "            merged_l.write(y)\n",
    "            index_to_pop = random.randint(0, len(buckets['data/neg_silver_samples_data.txt'])-1)\n",
    "            text_string = buckets['data/neg_silver_samples_data.txt'].pop(index_to_pop)\n",
    "            merged_d.write(text_string)\n",
    "            labels_string = \" \"\n",
    "            for a in range(len(text_string.split())):\n",
    "                labels_string += \"O \"\n",
    "            labels_string += '\\n'\n",
    "            merged_l.write(labels_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
